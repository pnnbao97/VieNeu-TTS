import gc
import os
import queue
import sys
import tempfile
import threading
import time
from functools import lru_cache

import gradio as gr
import numpy as np
import soundfile as sf
import torch

from app_config import (
    BACKBONE_CONFIGS,
    CODEC_CONFIGS,
    GGUF_ALLOWED_VOICES,
    MAX_CHARS_PER_CHUNK,
    SPEAKER_MODE_TO_KEY,
    VOICE_SAMPLES,
)
from utils.core_utils import split_text_into_chunks
from vieneu_tts import FastVieNeuTTS, VieNeuTTS

# Global model instance
tts = None
current_backbone = None
current_codec = None
model_loaded = False
using_lmdeploy = False


@lru_cache(maxsize=32)
def get_ref_text_cached(text_path: str) -> str:
    """Cache reference text loading"""
    with open(text_path, "r", encoding="utf-8") as f:
        return f.read()


def get_available_devices() -> list[str]:
    """Get list of available devices for current platform."""
    devices = ["Auto", "CPU"]

    if sys.platform == "darwin":
        if torch.backends.mps.is_available():
            devices.append("MPS")
    else:
        if torch.cuda.is_available():
            devices.append("CUDA")

    return devices


def should_use_lmdeploy(backbone_choice: str, device_choice: str) -> bool:
    """Determine if we should use LMDeploy backend."""
    if sys.platform == "darwin":
        return False

    if "gguf" in backbone_choice.lower():
        return False

    if device_choice == "Auto":
        has_gpu = torch.cuda.is_available()
    elif device_choice == "CUDA":
        has_gpu = torch.cuda.is_available()
    else:
        has_gpu = False

    return has_gpu


def cleanup_gpu_memory():
    """Aggressively cleanup GPU memory"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    elif torch.backends.mps.is_available():
        torch.mps.empty_cache()
    gc.collect()


def load_model(backbone_choice: str, codec_choice: str, device_choice: str,
               enable_triton: bool, max_batch_size: int):
    """Load model with optimizations and max batch size control"""
    global tts, current_backbone, current_codec, model_loaded, using_lmdeploy
    lmdeploy_error_reason = None

    yield (
        "‚è≥ ƒêang t·∫£i model v·ªõi t·ªëi ∆∞u h√≥a... L∆∞u √Ω: Qu√° tr√¨nh n√†y s·∫Ω t·ªën th·ªùi gian. Vui l√≤ng ki√™n nh·∫´n.",
        gr.update(interactive=False),
        gr.update(interactive=False)
    )

    try:
        if model_loaded and tts is not None:
            del tts
            cleanup_gpu_memory()

        backbone_config = BACKBONE_CONFIGS[backbone_choice]
        codec_config = CODEC_CONFIGS[codec_choice]

        use_lmdeploy = should_use_lmdeploy(backbone_choice, device_choice)

        if use_lmdeploy:
            lmdeploy_error_reason = None
            print("üöÄ Using LMDeploy backend with optimizations")

            backbone_device = "cuda"

            if "ONNX" in codec_choice:
                codec_device = "cpu"
            else:
                codec_device = "cuda" if torch.cuda.is_available() else "cpu"

            print("üöÄ Loading optimized model...")
            print(f"   Backbone: {backbone_config['repo']} on {backbone_device}")
            print(f"   Codec: {codec_config['repo']} on {codec_device}")
            print(f"   Triton: {'Enabled' if enable_triton else 'Disabled'}")
            print(f"   Max Batch Size: {max_batch_size}")

            try:
                tts = FastVieNeuTTS(
                    backbone_repo=backbone_config["repo"],
                    backbone_device=backbone_device,
                    codec_repo=codec_config["repo"],
                    codec_device=codec_device,
                    memory_util=0.3,
                    tp=1,
                    enable_prefix_caching=True,
                    enable_triton=enable_triton,
                    max_batch_size=max_batch_size,
                )
                using_lmdeploy = True

                print("üöÄ Pre-caching voice references...")
                for voice_name, voice_info in VOICE_SAMPLES.items():
                    audio_path = voice_info["audio"]
                    text_path = voice_info["text"]
                    if os.path.exists(audio_path) and os.path.exists(text_path):
                        ref_text = get_ref_text_cached(text_path)
                        tts.get_cached_reference(voice_name, audio_path, ref_text)
                print(f"   ‚úÖ Cached {len(VOICE_SAMPLES)} voices")

            except Exception as e:
                import traceback
                traceback.print_exc()

                error_str = str(e)
                if "$env:CUDA_PATH" in error_str:
                    lmdeploy_error_reason = "Kh√¥ng t√¨m th·∫•y bi·∫øn m√¥i tr∆∞·ªùng CUDA_PATH. Vui l√≤ng c√†i ƒë·∫∑t NVIDIA GPU Computing Toolkit."
                else:
                    lmdeploy_error_reason = f"{error_str}"

                yield (
                    f"‚ö†Ô∏è LMDeploy Init Error: {lmdeploy_error_reason}. ƒêang loading model v·ªõi backend m·∫∑c ƒë·ªãnh - t·ªëc ƒë·ªô ch·∫≠m h∆°n so v·ªõi lmdeploy...",
                    gr.update(interactive=False),
                    gr.update(interactive=False)
                )
                time.sleep(1)
                use_lmdeploy = False
                using_lmdeploy = False

        if not use_lmdeploy:
            print("üöÄ Using original backend")

            if device_choice == "Auto":
                if "gguf" in backbone_choice.lower():
                    if sys.platform == "darwin":
                        backbone_device = "gpu"
                    else:
                        backbone_device = "gpu" if torch.cuda.is_available() else "cpu"
                else:
                    if sys.platform == "darwin":
                        backbone_device = "mps" if torch.backends.mps.is_available() else "cpu"
                    else:
                        backbone_device = "cuda" if torch.cuda.is_available() else "cpu"

                if "ONNX" in codec_choice:
                    codec_device = "cpu"
                elif sys.platform == "darwin":
                    codec_device = "mps" if torch.backends.mps.is_available() else "cpu"
                else:
                    codec_device = "cuda" if torch.cuda.is_available() else "cpu"

            elif device_choice == "MPS":
                backbone_device = "mps"
                codec_device = "mps" if "ONNX" not in codec_choice else "cpu"

            else:
                backbone_device = device_choice.lower()
                codec_device = device_choice.lower()

                if "ONNX" in codec_choice:
                    codec_device = "cpu"

            if "gguf" in backbone_choice.lower() and backbone_device == "cuda":
                backbone_device = "gpu"

            print("üöÄ Loading model...")
            print(f"   Backbone: {backbone_config['repo']} on {backbone_device}")
            print(f"   Codec: {codec_config['repo']} on {codec_device}")

            tts = VieNeuTTS(
                backbone_repo=backbone_config["repo"],
                backbone_device=backbone_device,
                codec_repo=codec_config["repo"],
                codec_device=codec_device
            )
            using_lmdeploy = False

        current_backbone = backbone_choice
        current_codec = codec_choice
        model_loaded = True

        backend_name = "üöÄ LMDeploy (Optimized)" if using_lmdeploy else "üöÄ Standard"
        device_info = "cuda" if use_lmdeploy else (backbone_device if not use_lmdeploy else "N/A")

        streaming_support = "‚úÖ C√≥" if backbone_config['supports_streaming'] else "‚ùå Kh√¥ng"
        preencoded_note = "\n‚ö†Ô∏è Codec n√†y c·∫ßn s·ª≠ d·ª•ng pre-encoded codes (.pt files)" if codec_config['use_preencoded'] else ""

        opt_info = ""
        if using_lmdeploy and hasattr(tts, 'get_optimization_stats'):
            stats = tts.get_optimization_stats()
            opt_info = (
                "\n\n‚ö° T·ªëi ∆∞u h√≥a:"
                f"\n  ‚Ä¢ Triton: {'‚úÖ' if stats['triton_enabled'] else '‚ùå'}"
                f"\n  ‚Ä¢ Max Batch Size: {max_batch_size}"
                f"\n  ‚Ä¢ Reference Cache: {stats['cached_references']} voices"
                "\n  ‚Ä¢ Prefix Caching: ‚úÖ"
            )

        warning_msg = ""
        if lmdeploy_error_reason:
            warning_msg = (
                "\n\n‚ö†Ô∏è **C·∫£nh b√°o:** Kh√¥ng th·ªÉ k√≠ch ho·∫°t LMDeploy (Optimized Backend) do l·ªói sau:\n"
                f"‚ö†Ô∏è {lmdeploy_error_reason}\n"
                "‚ö†Ô∏è H·ªá th·ªëng ƒë√£ t·ª± ƒë·ªông chuy·ªÉn v·ªÅ ch·∫ø ƒë·ªô Standard (ch·∫≠m h∆°n)."
            )

        success_msg = (
            "‚úÖ Model ƒë√£ t·∫£i th√†nh c√¥ng!\n\n"
            f"üöÄ Backend: {backend_name}\n"
            f"ü¶ú Model Device: {device_info.upper()}\n"
            f"üéõ Codec Device: {codec_device.upper()}{preencoded_note}\n"
            f"üåä Streaming: {streaming_support}{opt_info}{warning_msg}"
        )

        yield (
            success_msg,
            gr.update(interactive=True),
            gr.update(interactive=True)
        )

    except Exception as e:
        import traceback
        traceback.print_exc()
        model_loaded = False
        using_lmdeploy = False

        if "$env:CUDA_PATH" in str(e):
            yield (
                "‚ùå L·ªói khi t·∫£i model: Kh√¥ng t√¨m th·∫•y bi·∫øn m√¥i tr∆∞·ªùng CUDA_PATH. Vui l√≤ng c√†i ƒë·∫∑t NVIDIA GPU Computing Toolkit (https://developer.nvidia.com/cuda/toolkit)",
                gr.update(interactive=False),
                gr.update(interactive=True)
            )
        else:
            yield (
                f"‚ùå L·ªói khi t·∫£i model: {str(e)}",
                gr.update(interactive=False),
                gr.update(interactive=True)
            )


def synthesize_speech(text: str, voice_choice: str, custom_audio, custom_text: str,
                     mode_tab: str, generation_mode: str, use_batch: bool):
    """Synthesis with optimization support and max batch size control"""
    global tts, current_backbone, current_codec, model_loaded, using_lmdeploy

    if not model_loaded or tts is None:
        yield None, "‚ö†Ô∏è Vui l√≤ng t·∫£i model tr∆∞·ªõc!"
        return

    if not text or text.strip() == "":
        yield None, "‚ö†Ô∏è Vui l√≤ng nh·∫≠p vƒÉn b·∫£n!"
        return

    raw_text = text.strip()

    codec_config = CODEC_CONFIGS[current_codec]
    use_preencoded = codec_config['use_preencoded']

    if mode_tab == "custom_mode":
        if custom_audio is None or not custom_text:
            yield None, "‚ö†Ô∏è Thi·∫øu Audio ho·∫∑c Text m·∫´u custom."
            return
        ref_audio_path = custom_audio
        ref_text_raw = custom_text
        ref_codes_path = None
    else:
        if voice_choice not in VOICE_SAMPLES:
            yield None, "‚ö†Ô∏è Vui l√≤ng ch·ªçn gi·ªçng m·∫´u."
            return
        ref_audio_path = VOICE_SAMPLES[voice_choice]["audio"]
        text_path = VOICE_SAMPLES[voice_choice]["text"]
        ref_codes_path = VOICE_SAMPLES[voice_choice]["codes"]

        if not os.path.exists(ref_audio_path):
            yield None, "‚ùå Kh√¥ng t√¨m th·∫•y file audio m·∫´u."
            return

        ref_text_raw = get_ref_text_cached(text_path)

    yield None, "üîÑ ƒêang x·ª≠ l√Ω Reference..."

    try:
        if use_preencoded and ref_codes_path and os.path.exists(ref_codes_path):
            ref_codes = torch.load(ref_codes_path, map_location="cpu", weights_only=True)
        else:
            if using_lmdeploy and hasattr(tts, 'get_cached_reference') and mode_tab == "preset_mode":
                ref_codes = tts.get_cached_reference(voice_choice, ref_audio_path, ref_text_raw)
            else:
                ref_codes = tts.encode_reference(ref_audio_path)

        if isinstance(ref_codes, torch.Tensor):
            ref_codes = ref_codes.cpu().numpy()
    except Exception as e:
        yield None, f"‚ùå L·ªói x·ª≠ l√Ω reference: {e}"
        return

    text_chunks = split_text_into_chunks(raw_text, max_chars=MAX_CHARS_PER_CHUNK)
    total_chunks = len(text_chunks)

    if generation_mode == "Standard (M·ªôt l·∫ßn)":
        backend_name = "LMDeploy" if using_lmdeploy else "Standard"
        batch_info = " (Batch Mode)" if use_batch and using_lmdeploy and total_chunks > 1 else ""

        batch_size_info = ""
        if use_batch and using_lmdeploy and hasattr(tts, 'max_batch_size'):
            batch_size_info = f" [Max batch: {tts.max_batch_size}]"

        yield None, f"üöÄ B·∫Øt ƒë·∫ßu t·ªïng h·ª£p {backend_name}{batch_info}{batch_size_info} ({total_chunks} ƒëo·∫°n)..."

        all_audio_segments = []
        sr = 24000
        silence_pad = np.zeros(int(sr * 0.15), dtype=np.float32)

        start_time = time.time()

        try:
            if use_batch and using_lmdeploy and hasattr(tts, 'infer_batch') and total_chunks > 1:
                batch_size = tts.max_batch_size if hasattr(tts, 'max_batch_size') else 8
                num_batches = (total_chunks + batch_size - 1) // batch_size

                yield None, f"‚ö° X·ª≠ l√Ω {num_batches} mini-batch(es) (max {batch_size} ƒëo·∫°n/batch)..."

                chunk_wavs = tts.infer_batch(text_chunks, ref_codes, ref_text_raw)

                for i, chunk_wav in enumerate(chunk_wavs):
                    if chunk_wav is not None and len(chunk_wav) > 0:
                        all_audio_segments.append(chunk_wav)
                        if i < total_chunks - 1:
                            all_audio_segments.append(silence_pad)
            else:
                for i, chunk in enumerate(text_chunks):
                    yield None, f"‚è≥ ƒêang x·ª≠ l√Ω ƒëo·∫°n {i+1}/{total_chunks}..."

                    chunk_wav = tts.infer(chunk, ref_codes, ref_text_raw)

                    if chunk_wav is not None and len(chunk_wav) > 0:
                        all_audio_segments.append(chunk_wav)
                        if i < total_chunks - 1:
                            all_audio_segments.append(silence_pad)

            if not all_audio_segments:
                yield None, "‚ùå Kh√¥ng sinh ƒë∆∞·ª£c audio n√†o."
                return

            yield None, "üîÑ ƒêang gh√©p file v√† l∆∞u..."

            final_wav = np.concatenate(all_audio_segments)
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
                sf.write(tmp.name, final_wav, sr)
                output_path = tmp.name

            process_time = time.time() - start_time
            backend_info = f" (Backend: {'LMDeploy üöÄ' if using_lmdeploy else 'Standard ‚ö°'})"
            speed_info = f", T·ªëc ƒë·ªô: {len(final_wav)/sr/process_time:.2f}x realtime" if process_time > 0 else ""

            yield output_path, f"‚úÖ Ho√†n t·∫•t! (Th·ªùi gian: {process_time:.2f}s{speed_info}){backend_info}"

            if using_lmdeploy and hasattr(tts, 'cleanup_memory'):
                tts.cleanup_memory()
            cleanup_gpu_memory()

        except torch.cuda.OutOfMemoryError as e:
            cleanup_gpu_memory()
            yield None, (
                "‚ùå GPU h·∫øt VRAM! H√£y th·ª≠:\n"
                f"‚Ä¢ Gi·∫£m Max Batch Size (hi·ªán t·∫°i: {tts.max_batch_size if hasattr(tts, 'max_batch_size') else 'N/A'})\n"
                "‚Ä¢ Gi·∫£m ƒë·ªô d√†i vƒÉn b·∫£n\n\n"
                f"Chi ti·∫øt: {str(e)}"
            )
            return

        except Exception as e:
            import traceback
            traceback.print_exc()
            cleanup_gpu_memory()
            yield None, f"‚ùå L·ªói Standard Mode: {str(e)}"
            return

    else:
        sr = 24000
        crossfade_samples = int(sr * 0.03)
        audio_queue = queue.Queue(maxsize=100)
        PRE_BUFFER_SIZE = 3

        end_event = threading.Event()
        error_event = threading.Event()
        error_msg = ""

        def producer_thread():
            nonlocal error_msg
            try:
                previous_tail = None

                for chunk_text in text_chunks:
                    stream_gen = tts.infer_stream(chunk_text, ref_codes, ref_text_raw)

                    for audio_part in stream_gen:
                        if audio_part is None or len(audio_part) == 0:
                            continue

                        if previous_tail is not None and len(previous_tail) > 0:
                            overlap = min(len(previous_tail), len(audio_part), crossfade_samples)
                            if overlap > 0:
                                fade_out = np.linspace(1.0, 0.0, overlap, dtype=np.float32)
                                fade_in = np.linspace(0.0, 1.0, overlap, dtype=np.float32)

                                blended = (audio_part[:overlap] * fade_in +
                                           previous_tail[-overlap:] * fade_out)

                                processed = np.concatenate([
                                    previous_tail[:-overlap] if len(previous_tail) > overlap else np.array([]),
                                    blended,
                                    audio_part[overlap:]
                                ])
                            else:
                                processed = np.concatenate([previous_tail, audio_part])

                            tail_size = min(crossfade_samples, len(processed))
                            previous_tail = processed[-tail_size:].copy()
                            output_chunk = processed[:-tail_size] if len(processed) > tail_size else processed
                        else:
                            tail_size = min(crossfade_samples, len(audio_part))
                            previous_tail = audio_part[-tail_size:].copy()
                            output_chunk = audio_part[:-tail_size] if len(audio_part) > tail_size else audio_part

                        if len(output_chunk) > 0:
                            audio_queue.put((sr, output_chunk))

                if previous_tail is not None and len(previous_tail) > 0:
                    audio_queue.put((sr, previous_tail))

            except Exception as e:
                import traceback
                traceback.print_exc()
                error_msg = str(e)
                error_event.set()
            finally:
                end_event.set()
                audio_queue.put(None)

        threading.Thread(target=producer_thread, daemon=True).start()

        yield (sr, np.zeros(int(sr * 0.05))), "üîÑ ƒêang buffering..."

        pre_buffer = []
        while len(pre_buffer) < PRE_BUFFER_SIZE:
            try:
                item = audio_queue.get(timeout=5.0)
                if item is None:
                    break
                pre_buffer.append(item)
            except queue.Empty:
                if error_event.is_set():
                    yield None, f"‚ùå L·ªói: {error_msg}"
                    return
                break

        full_audio_buffer = []
        backend_info = "üöÄ LMDeploy" if using_lmdeploy else "‚ö° Standard"
        for sr, audio_data in pre_buffer:
            full_audio_buffer.append(audio_data)
            yield (sr, audio_data), f"üîä ƒêang ph√°t ({backend_info})..."

        while True:
            try:
                item = audio_queue.get(timeout=0.05)
                if item is None:
                    break
                sr, audio_data = item
                full_audio_buffer.append(audio_data)
                yield (sr, audio_data), f"üîä ƒêang ph√°t ({backend_info})..."
            except queue.Empty:
                if error_event.is_set():
                    yield None, f"‚ùå L·ªói: {error_msg}"
                    break
                if end_event.is_set() and audio_queue.empty():
                    break
                continue

        if full_audio_buffer:
            final_wav = np.concatenate(full_audio_buffer)
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
                sf.write(tmp.name, final_wav, sr)
                yield tmp.name, f"‚úÖ Ho√†n t·∫•t Streaming! ({backend_info})"

            if using_lmdeploy and hasattr(tts, 'cleanup_memory'):
                tts.cleanup_memory()
            cleanup_gpu_memory()


def _parse_line_order(line_order, line_count):
    order = []
    if isinstance(line_order, str):
        for part in line_order.split(","):
            part = part.strip()
            if part.isdigit():
                idx = int(part)
                if 1 <= idx <= line_count and idx not in order:
                    order.append(idx)
    if not order:
        order = list(range(1, line_count + 1))
    else:
        for idx in range(1, line_count + 1):
            if idx not in order:
                order.append(idx)
    return order


def synthesize_multi_voice(line_count, line_order, use_batch, *line_inputs):
    global tts, current_backbone, current_codec, model_loaded, using_lmdeploy

    if not model_loaded or tts is None:
        yield None, "Vui l√≤ng t·∫£i model tr∆∞·ªõc."
        return

    try:
        line_count = int(line_count)
    except (TypeError, ValueError):
        line_count = 0

    if line_count <= 0:
        yield None, "Vui l√≤ng th√™m ng∆∞·ªùi n√≥i."
        return

    line_fields = 5
    segments = []
    order = _parse_line_order(line_order, line_count)

    for line_idx in order:
        base = (line_idx - 1) * line_fields
        if base + line_fields > len(line_inputs):
            continue
        mode_label = line_inputs[base]
        voice_choice = line_inputs[base + 1]
        custom_audio = line_inputs[base + 2]
        custom_text = line_inputs[base + 3]
        line_text = line_inputs[base + 4]

        if not line_text or not str(line_text).strip():
            continue

        segments.append({
            "mode": SPEAKER_MODE_TO_KEY.get(mode_label, "preset"),
            "voice_choice": voice_choice,
            "custom_audio": custom_audio,
            "custom_text": custom_text,
            "text": str(line_text).strip(),
        })

    if not segments:
        yield None, "Vui l√≤ng nh·∫≠p n·ªôi dung h·ªôi tho·∫°i."
        return

    codec_config = CODEC_CONFIGS[current_codec]
    use_preencoded = codec_config["use_preencoded"]

    yield None, "ƒêang chu·∫©n b·ªã m·∫´u tham chi·∫øu..."

    ref_cache = {}
    try:
        for segment in segments:
            mode_key = segment["mode"]
            voice_choice = segment["voice_choice"]
            custom_audio = segment["custom_audio"]
            custom_text = segment["custom_text"] or ""

            if mode_key == "preset":
                cache_key = ("preset", voice_choice)
            else:
                cache_key = ("custom", custom_audio, custom_text)

            if cache_key in ref_cache:
                continue

            if mode_key == "preset":
                if voice_choice not in VOICE_SAMPLES:
                    yield None, f"Gi·ªçng m·∫´u kh√¥ng h·ª£p l·ªá: {voice_choice}"
                    return
                if "gguf" in current_backbone.lower() and voice_choice not in GGUF_ALLOWED_VOICES:
                    yield None, f"Gi·ªçng kh√¥ng h·ªó tr·ª£ cho GGUF: {voice_choice}"
                    return

                ref_audio_path = VOICE_SAMPLES[voice_choice]["audio"]
                text_path = VOICE_SAMPLES[voice_choice]["text"]
                ref_codes_path = VOICE_SAMPLES[voice_choice]["codes"]

                if not os.path.exists(ref_audio_path):
                    yield None, f"Thi·∫øu audio gi·ªçng m·∫´u: {voice_choice}"
                    return

                ref_text_raw = get_ref_text_cached(text_path)
            else:
                ref_audio_path = custom_audio
                ref_text_raw = custom_text
                ref_codes_path = None

                if not ref_audio_path or not ref_text_raw:
                    yield None, "Thi·∫øu audio ho·∫∑c vƒÉn b·∫£n cho gi·ªçng t√πy ch·ªânh."
                    return
                if not os.path.exists(ref_audio_path):
                    yield None, "Kh√¥ng t√¨m th·∫•y file audio gi·ªçng t√πy ch·ªânh."
                    return

            if use_preencoded and ref_codes_path and os.path.exists(ref_codes_path):
                ref_codes = torch.load(ref_codes_path, map_location="cpu", weights_only=True)
            else:
                if mode_key == "preset" and using_lmdeploy and hasattr(tts, "get_cached_reference"):
                    ref_codes = tts.get_cached_reference(voice_choice, ref_audio_path, ref_text_raw)
                else:
                    ref_codes = tts.encode_reference(ref_audio_path)

            if isinstance(ref_codes, torch.Tensor):
                ref_codes = ref_codes.cpu().numpy()

            ref_cache[cache_key] = {
                "codes": ref_codes,
                "ref_text": ref_text_raw,
            }
    except Exception as e:
        yield None, f"L·ªói tham chi·∫øu: {e}"
        return

    total_segments = len(segments)
    sr = 24000
    chunk_silence = np.zeros(int(sr * 0.15), dtype=np.float32)
    segment_silence = np.zeros(int(sr * 0.2), dtype=np.float32)

    all_audio_segments = []
    start_time = time.time()

    try:
        for idx, segment in enumerate(segments):
            mode_key = segment["mode"]
            text = segment["text"]
            if mode_key == "preset":
                cache_key = ("preset", segment["voice_choice"])
            else:
                cache_key = ("custom", segment["custom_audio"], segment["custom_text"] or "")

            ref_codes = ref_cache[cache_key]["codes"]
            ref_text_raw = ref_cache[cache_key]["ref_text"]

            text_chunks = split_text_into_chunks(text, max_chars=MAX_CHARS_PER_CHUNK)
            yield None, f"ƒêang x·ª≠ l√Ω {idx+1}/{total_segments} ({len(text_chunks)} ƒëo·∫°n)..."

            segment_audio_parts = []

            if use_batch and using_lmdeploy and hasattr(tts, "infer_batch") and len(text_chunks) > 1:
                chunk_wavs = tts.infer_batch(text_chunks, ref_codes, ref_text_raw)
                for i, chunk_wav in enumerate(chunk_wavs):
                    if chunk_wav is not None and len(chunk_wav) > 0:
                        segment_audio_parts.append(chunk_wav)
                        if i < len(text_chunks) - 1:
                            segment_audio_parts.append(chunk_silence)
            else:
                for i, chunk in enumerate(text_chunks):
                    chunk_wav = tts.infer(chunk, ref_codes, ref_text_raw)
                    if chunk_wav is not None and len(chunk_wav) > 0:
                        segment_audio_parts.append(chunk_wav)
                        if i < len(text_chunks) - 1:
                            segment_audio_parts.append(chunk_silence)

            if not segment_audio_parts:
                continue

            if all_audio_segments:
                all_audio_segments.append(segment_silence)
            all_audio_segments.append(np.concatenate(segment_audio_parts))

        if not all_audio_segments:
            yield None, "Kh√¥ng sinh ƒë∆∞·ª£c audio."
            return

        yield None, "ƒêang ho√†n t·∫•t audio..."
        final_wav = np.concatenate(all_audio_segments)
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
            sf.write(tmp.name, final_wav, sr)
            output_path = tmp.name

        process_time = time.time() - start_time
        backend_info = f" (Backend: {'LMDeploy' if using_lmdeploy else 'Standard'})"
        speed_info = f", T·ªëc ƒë·ªô: {len(final_wav)/sr/process_time:.2f}x realtime" if process_time > 0 else ""

        yield output_path, f"Ho√†n t·∫•t! (Th·ªùi gian: {process_time:.2f}s{speed_info}){backend_info}"

        if using_lmdeploy and hasattr(tts, "cleanup_memory"):
            tts.cleanup_memory()
        cleanup_gpu_memory()

    except torch.cuda.OutOfMemoryError as e:
        cleanup_gpu_memory()
        yield None, (
            "GPU h·∫øt VRAM. H√£y gi·∫£m max batch size ho·∫∑c ƒë·ªô d√†i vƒÉn b·∫£n.\n\n"
            f"Chi ti·∫øt: {str(e)}"
        )
        return
    except Exception as e:
        import traceback
        traceback.print_exc()
        cleanup_gpu_memory()
        yield None, f"L·ªói ƒëa gi·ªçng: {str(e)}"
        return


def synthesize_router(text, voice_choice, custom_audio, custom_text,
                      mode_tab, generation_mode, use_batch,
                      multi_line_count, multi_line_order, *multi_line_inputs):
    if mode_tab == "multi_mode":
        yield from synthesize_multi_voice(multi_line_count, multi_line_order, use_batch, *multi_line_inputs)
    else:
        yield from synthesize_speech(text, voice_choice, custom_audio, custom_text, mode_tab, generation_mode, use_batch)
